{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  ATOC5860 Application Lab #2 - AR1_regression_AO\n",
    "##### Originally coded by Prof. Kay (CU) and Elizabeth Maroon (CU postdoc, now U.Wisc)\n",
    "##### last updated January 31, 2024\n",
    "\n",
    "### LEARNING GOALS:\n",
    "1) Calculate and analyze the autocorrelation at a range of lags using output from an EOF analysis (the Arctic Oscillation Index).\n",
    "2) Generate a red noise time series with equivalent memory as an observed time series (i.e., given lag-1 autocorrelation).\n",
    "3) Correlate two time series and calculate the statistical significance.\n",
    "4) Evaluate the statistical significance obtained in the context of the number of chances provided for success.  What happens when you go “fishing” for correlations and give yourself lots of opportunity for success?  Can you critically evaluate the chances that your regression is statistically different than 0 just by chance?\n",
    "\n",
    "### DATA and UNDERLYING SCIENCE MOTIVATION:  \n",
    "In this notebook, you will analyze the monthly Arctic Oscillation (AO) timeseries from January 1950 to present. The AO timeseries comes from an Empirical Orthogonal Function (EOF) analysis. We will implement EOFs in the next unit in ATOC5860. Yea! In this lab we are actually using multiple analysis methods introduced in this class, some that you have learned and some that you are still yet to learn.\n",
    "\n",
    "How do you find the AO value each month?  To identify the atmospheric circulation patterns that explain the most variance, NOAA regularly applies EOF analysis to the monthly mean 1000-hPa height anomalies poleward of 20° latitude for the Northern Hemisphere. The AO spatial pattern (Figure 1 below) emerges as the first EOF (explaining the most variance, 19%). The AO timeseries we will analyze is a measure of the amplitude of the pattern in Figure 1 in a given month.  In other words – the AO timeseries is the first principal component (a timeseries) associated with the first EOF (a spatial structure). More information on the EOF analysis __[here](http://www.cpc.ncep.noaa.gov/products/precip/CWlink/daily_ao_index/history/method.shtml)__\n",
    "\n",
    "![model_pipeline](./lab2_figure1.png)\n",
    "\n",
    "The data are available and regularly updated  __[here](http://www.cpc.ncep.noaa.gov/products/precip/CWlink/pna/norm.nao.monthly.b5001.current.ascii)__\n",
    "\n",
    "You can work with the data directly on the web (assuming you have an internet connection). I have also downloaded the data and made them available. The name of the data file is **monthly.ao.index.b50.current.ascii**.\n",
    "\n",
    "### Non-exhaustive Questions to guide your analysis:  \n",
    "\n",
    "1) Start with the default settings in the code.  First read in the Arctic Oscillation (AO) data.  Look at your data!!  Plot it as a timeseries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import xarray\n",
    "import pandas as pd   ##pandas reads in .csv.\n",
    "from scipy import stats\n",
    "import seaborn as sns  ### seaborn just makes things pretty -- no new typing\n",
    "sns.set_style('whitegrid')  ###  change away from grey grid (seaborn default) to white grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Define function.  \n",
    "### Functions can be really useful for compact code. Feel free to expand. \n",
    "### But double and triple check functions when you write them, functions can obscure what the code is doing.\n",
    "### We will tend to not use them in ATOC5860 because we always wto have eyes on the code we are running.\n",
    "def ar1_series(a,N):\n",
    "    red_series = []\n",
    "    red_series.append(1)\n",
    "    for t in range(1,N,1):\n",
    "        red_series.append(a*red_series[t-1] + np.sqrt(1-np.power(a,2))*stats.norm.rvs(size=1)) ##Barnes Ch. 2 Eq. 69\n",
    "    return_red_series = np.vstack(red_series)[:,0] ## convert to a numpy float array\n",
    "    return return_red_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### This notebook uses data from the Arctic Oscillation regularly updated by NOAA\n",
    "### For more information go here: \n",
    "### http://www.cpc.ncep.noaa.gov/products/precip/CWlink/daily_ao_index/history/method.shtml\n",
    "\n",
    "## Load in AO timeseries from the web, Look at it, and Plot it\n",
    "\n",
    "## Note this will not work if you do not have an internet connection as it is directly grabbing the data from the web.\n",
    "#data = pd.read_csv('http://www.cpc.ncep.noaa.gov/products/precip/CWlink/daily_ao_index/monthly.ao.index.b50.current.ascii',\\\n",
    "#                   sep='\\s+',header=None, names = ['year', 'month','AO'])\n",
    "## if not on-line\n",
    "data = pd.read_csv('monthly.ao.index.b50.current.ascii',\\\n",
    "                   sep='\\s+',header=None, names = ['year', 'month','AO'])\n",
    "\n",
    "data['time']=data['year']+data['month']/12-1/24\n",
    "#print(data)\n",
    "plt.plot(data['time'],data['AO'])\n",
    "plt.savefig('AO_timeseries.eps')  ## save your plot as a postscript file\n",
    "plt.ylabel('Monthy AO Timeseries')\n",
    "print(data.head());\n",
    "print(data.tail());"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Calculate the lag-one autocorrelation (AR1) of the AO data and record it here. Use two methods (np.correlate, dot products).  Check that they give you the same result.  Interpret the value.  How much memory (red noise) is there in the AO from month to month?\n",
    "\n",
    "30% autocorrelation, not statistically significant?, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Calculate the lag-one autocorrelation (AR1) of the AO data and print it to the screen\n",
    "## Use two methods (np.correlate, dot products).  They should give you the same result.\n",
    "time=data['time']\n",
    "tseries=data['AO']\n",
    "sigma=np.std(tseries)  ## calculate the standard deviation\n",
    "mean=np.mean(tseries)  ## calculate the mean\n",
    "n=len(tseries)         ## calculate the length of the timeseries\n",
    "lag=1\n",
    "\n",
    "##Create two timeseries of the data at t=t1 and t=t2; remove the mean\n",
    "t1_m=tseries[0:-1*lag]-mean\n",
    "t2_m=tseries[lag:]-mean\n",
    "\n",
    "#Method #1\n",
    "#Calculate the autocorrelation using numpy correlate lagN\n",
    "lagNauto_np=np.correlate(t1_m,t2_m,mode='valid')/(n-lag)/(sigma**2)  ## Eq. 67 divided by the variance\n",
    "print('np.correlate autocorrelation: ',np.round(lagNauto_np,5))\n",
    "\n",
    "#Method #2 (should be faster)\n",
    "#Calculate the autocorrelation using np.dot (https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.dot.html)\n",
    "lagNauto=np.dot(t1_m,t2_m)/(n-lag)/sigma**2 ## Eq. 67 divided by the variance\n",
    "print('direct calculation autocorrelation:',np.round(lagNauto,5))\n",
    "\n",
    "if lag==1: ar1=lagNauto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) Calculate and plot the autocorrelation of the AO data at all lags.  Describe your results.  How red are the data at lags other than lag=1?  Is there any interesting behavior of the autocorrelation as a function of lag?  What would you expect for red noise timeseries with an AR1=value reported in 2)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Calculate and plot the autocorrelation at all lags\n",
    "tseries1=tseries-mean\n",
    "half=int(len(tseries)/2)\n",
    "lags=np.arange(-1*half,half,1.)\n",
    "\n",
    "## note - mode='same' is supposed to be chopping tseries1 so that it has the same length even when lagged.\n",
    "## in principal this should work but it wasn't working for me.\n",
    "#plt.plot(range(-1*half,half),np.correlate(tseries1,tseries1,mode='same')/((n-lags)*sigma**2)) ##Eq. 67 divided by variance\n",
    "\n",
    "## same as above but ensuring that the lags and autocorrelations have the same length for plotting\n",
    "foo=np.correlate(tseries1,tseries1,mode='same')\n",
    "#print(np.shape(foo))\n",
    "foo1=np.shape(lags)\n",
    "#print(foo1)\n",
    "plt.plot(lags,foo[0:foo1[0]]/((n-np.abs(lags))*sigma**2)) ##Eq. 67 divided by variance\n",
    "\n",
    "plt.xlim([-10,10])\n",
    "plt.xlabel('Lag (Months)');\n",
    "plt.ylabel('Autocorrelation');\n",
    "plt.title('AO Autocorrelation at a range of lags');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4) Generate a synthetic red noise time series with the same lag-1 autocorrelation as the AO data. Your synthetic dataset should have different time evolution but the same memory as the AO.  Plot the AO timeseries and the synthetic red noise time series. Put the plot below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Generate red noise time series with the same lag-1 autocorrelation as the AO\n",
    "## In other words - generate a synthetic dataset with the same memory as the AO.\n",
    "\n",
    "tsnum=2 ## how many red noise time series to generate\n",
    "numred=len(time) ## length of red noise time series\n",
    "t_1=np.empty((tsnum,numred),dtype=float)\n",
    "colors=sns.color_palette('husl',tsnum) ##palette of colors\n",
    "f=plt.figure(figsize=(20,6))\n",
    "for nn in range(tsnum):\n",
    "    t_1[nn,:] = ar1_series(ar1,numred)\n",
    "    plt.plot(time,t_1[nn,:],color=colors[nn],label='Red Noise Timeseries with AR1='+str(np.round(ar1,3)))\n",
    "\n",
    "##Plot the two timeseries - the synthetic red noise and the actual data    \n",
    "plt.plot(time,(tseries-tseries.mean())/tseries.std(),color='black',label='AO data');\n",
    "plt.ylabel('Normalized Value',fontsize=14);\n",
    "plt.title('Compare synthetic red noise timeseries with observed AO timeseries',fontsize=18);\n",
    "plt.legend(loc='upper left',fontsize=14);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5) Do you expect to find any correlation between the two datasets, i.e., the synthetic red noise and the actual AO data? What is the correlation between the synthetic red noise and the actual AO data?  Calculate a regression coefficient and other associated regression statistics. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### What is the correlation between the synthetic red noise and the actual AO data?\n",
    "rednoise=t_1[0,:]\n",
    "data=(tseries-tseries.mean())/tseries.std()\n",
    "slope, intercept, r_value, p_value, std_err = stats.linregress(rednoise,data)\n",
    "variance_explained_all=(r_value*r_value)*100.\n",
    "print('What is percent variance explained?:',round(variance_explained_all,4),'%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6) Next -- Have some fun and go “fishing for correlations”.   What happens if you try correlating subsets of the two datasets many times?  When you try 200 times -- what is the maximum correlation/variance explained you can obtain between the synthetic red noise and the actual data?  *Note: you are effectively searching for a high correlation with no a priori reason to do so.... **THIS IS NOT good practice for science** but we are doing it here because it is instructive to see what happens :)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Let's go \"fishing for correlations\"....\n",
    "### What happens if you try correlating subsets of the two datasets?  \n",
    "### What is the correlation between the synthetic red noise and the actual data?\n",
    "### Can you get higher variance explained?\n",
    "### Note: you are effectively searching for a high correlation with no a priori reason to do so.... \n",
    "### THIS IS NOT good practice for science but interesting to see the results :)\n",
    "\n",
    "largest_variance=0\n",
    "largest_r=0\n",
    "nn_largest_variance=0\n",
    "length=20\n",
    "offset=3\n",
    "N_fishingforcorrelation=200 ## number of times you will go \"fishing for correlations\"\n",
    "for nn in range(N_fishingforcorrelation):\n",
    "    subset_rednoise=rednoise[nn*offset:nn*offset+length]\n",
    "    subset_data=data[nn*offset:nn*offset+length]\n",
    "    slope, intercept, r_value, p_value, std_err = stats.linregress(subset_rednoise,subset_data)\n",
    "    variance_explained=(r_value*r_value)*100.\n",
    "    if variance_explained > largest_variance: \n",
    "        largest_variance=variance_explained\n",
    "        largest_r=r_value\n",
    "        nn_largest_variance=nn\n",
    "    \n",
    "print('nn_largest_variance',nn_largest_variance)\n",
    "print('Largest r_value',round(largest_r,2))        \n",
    "print('Largest variance explained',round(largest_variance,2),'%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7)  Calculate the correlation statistics for the highest correlation obtained in question 6). Two methods are provided - they should give you the same answers. Place a confidence interval on your correlation. Because you have found a correlation that is not equal to 0, use the Fisher-Z Transformation. Did your \"fishing\" for a statistically significant correlation work?  Is your highest correlation statistically significant (i.e., can you reject the null hypothesis that the correlation is zero)?  Write out the steps for hypothesis testing and use the values you calculate to formally assess."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### You were searching for correlations, You searched long and hard (200 times!)\n",
    "### You might expect that the largest correlation you found will be a false positive.  \n",
    "### Is it?  Did your \"fishing\" for a high correlation work?\n",
    "\n",
    "## data chunks with the largest correlation\n",
    "fp_rednoise=rednoise[nn_largest_variance*offset:nn_largest_variance*offset+length]\n",
    "fp_data=data[nn_largest_variance*offset:nn_largest_variance*offset+length]\n",
    "\n",
    "## make a scatter plot - Do you see a correlation?\n",
    "plt.scatter(fp_data,fp_rednoise);   \n",
    "plt.xlabel('fp_data - the AO data');\n",
    "plt.ylabel('fp_rednoise - the synthetic red noise data');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Calculate correlation statistics for your highest correlation \n",
    "#### Two methods are shown - they should give you the same answers.\n",
    "\n",
    "### Calculate the correlation statistics - slow way, but convenient\n",
    "slope, intercept, r_value, p_value, std_err = stats.linregress(fp_rednoise,fp_data)\n",
    "print('scipy.stats.linregress slope: ',round(slope,3))\n",
    "print('scipy.stats.linregress intercept: ',round(intercept,3))\n",
    "print('scipy.stats.linregress r_value: ',round(r_value,3))\n",
    "\n",
    "### Calculate the correlation statistics - direct and fast way\n",
    "### for reference, answers should be the same...\n",
    "### Shows direct calculation using Barnes notes\n",
    "xdata=fp_rednoise-np.mean(fp_rednoise)\n",
    "ydata=fp_data-np.mean(fp_data)\n",
    "variancex=np.sum((xdata*xdata))\n",
    "covariance=np.dot(xdata,ydata)\n",
    "slope_fast=covariance/variancex                          ##Barnes Chapter 2, Eq. (14)\n",
    "intercept_fast=np.mean(fp_data)-slope_fast*np.mean(fp_rednoise)  ##Barnes Chapter 2, Eq. (15)\n",
    "variancey=np.sum((ydata*ydata))\n",
    "rvalue_fast=np.dot(xdata,ydata)/(np.sqrt(variancex)*np.sqrt(variancey)) ##Barnes Chapter 2, Eq. (31)\n",
    "print('direct method slope_fast: ',round(slope_fast,3))\n",
    "print('direct method intercept_fast: ',round(intercept_fast,3))\n",
    "print('direct method rvalue_fast: ',round(rvalue_fast,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Place a confidence interval on your correlation \n",
    "## Because you have found a correlation that is not equal to 0, use the Fisher-Z Transformation.\n",
    "\n",
    "## Calculate the Fisher-Z Transformation mean, Barnes Ch. 2 Eq. 52\n",
    "mu_Z=0.5*np.log((1+r_value)/(1-r_value)) \n",
    "#print('mu_Z:',mu_Z)\n",
    "df=len(fp_data)-3     ## Calculate the degrees of freedom\n",
    "sigma_Z=1/np.sqrt(df) ## Calculate the Fisher-Z Transformation standard deviation Barnes Ch. 2 Equation 53\n",
    "\n",
    "## Calculate the critical value, i.e., the t-statistic\n",
    "## Note: Python calculates left/lower-tail probabilities by default, so for the 95% confidence interval, use 0.975\n",
    "tstat95=stats.t.ppf(0.975,df)\n",
    "## Calculate the confidence intervals on the mean (mu_Z)\n",
    "Zmin=mu_Z-tstat95*sigma_Z ## Barnes Ch. 2 Equation 54\n",
    "Zmax=mu_Z+tstat95*sigma_Z ## Barnes Ch. 2 Equation 54\n",
    "#print('Zmin',Zmin)\n",
    "#print('Zmax',Zmax)\n",
    "\n",
    "## Convert to give a confidence interval on the correlation using Barnes Ch. 2 Eq. 55\n",
    "rho_min=np.tanh(Zmin)\n",
    "rho_max=np.tanh(Zmax)\n",
    "print('Rhomin (minimum 95% confidence interval for r_value):',round(rho_min,2))\n",
    "print('Rhomax (maximum 95% confidence interval for r_value):',round(rho_max,2))\n",
    "\n",
    "## If the confidence range for the true correlation given by Rhomin,Rhomax does not overlap with 0 -\n",
    "## we can reject the null hypothesis that the correlation between fp_data and fp_rednoise is zero.\n",
    "## If we reject the null hypothesis that the correlation is zero, we have found a false positive!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8) You went searching for correlations, you searched long and hard (200 times!) You should have been concerned that the largest correlation you found would be a false positive.  Do you think you found a false positive?  Explain what you found and potentially why you think it is important statistically but not physically.  What lessons did you learn by “fishing for correlations”?\n",
    "\n",
    "FOR FUN:  Check out these __[spurious correlations](https://www.tylervigen.com/spurious-correlations)__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Remember the Arctic warming blocking example from Barnes Chapter 1 applying a posteriori statistics?\n",
    "### How likely was it that you would correctly reject H0 for all of the correlations you explored?  \n",
    "### In other words - How likely are you to be correct in all of your assessments?\n",
    "### You should find it is very unlikely... and this should make you wary of doing anything like what we\n",
    "### were doing above when analyzing data and looking for correlations.\n",
    "prob_correctly_rejecting_all=(0.95**N_fishingforcorrelation)*100\n",
    "print('prob_correctly_rejecting_all',round(prob_correctly_rejecting_all,4),'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
